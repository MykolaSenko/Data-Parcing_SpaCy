{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOwload model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"source/Input_File_01.txt\", 'rb') as f:\n",
    "        content = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at 'source_data/Input_File_01.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext = (\"When Sebastian Thrun started working on self-driving cars at \"\\n        \"Google in 2007, few people outside of the company took him \"\\n        \"seriously. “I can tell you very senior CEOs of major American \"\\n        \"car companies would shake my hand and turn away because I wasn’t \"\\n        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\\n        \"this week.\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load English model\n",
    "\"\"\"\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExtraData",
     "evalue": "unpack(b) received extra data.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mExtraData\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/spacy/language.py:1041\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1021\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1022\u001b[39m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1025\u001b[39m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1026\u001b[39m ) -> Doc:\n\u001b[32m   1027\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[33;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[32m   1029\u001b[39m \u001b[33;03m    is preserved.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1039\u001b[39m \u001b[33;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m     doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1043\u001b[39m         component_cfg = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/spacy/language.py:1134\u001b[39m, in \u001b[36mLanguage._ensure_doc\u001b[39m\u001b[34m(self, doc_like)\u001b[39m\n\u001b[32m   1132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.make_doc(doc_like)\n\u001b[32m   1133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDoc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E1041.format(\u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/spacy/tokens/doc.pyx:1362\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.from_bytes\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/srsly/_msgpack_api.py:27\u001b[39m, in \u001b[36mmsgpack_loads\u001b[39m\u001b[34m(data, use_list)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# msgpack-python docs suggest disabling gc before unpacking large messages\u001b[39;00m\n\u001b[32m     26\u001b[39m gc.disable()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m msg = \u001b[43mmsgpack\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m gc.enable()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m msg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/srsly/msgpack/__init__.py:85\u001b[39m, in \u001b[36munpackb\u001b[39m\u001b[34m(packed, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         object_hook = functools.partial(decoder, chain=object_hook)\n\u001b[32m     84\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mobject_hook\u001b[39m\u001b[33m\"\u001b[39m] = object_hook\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_unpackb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/srsly/msgpack/_unpacker.pyx:213\u001b[39m, in \u001b[36msrsly.msgpack._unpacker.unpackb\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mExtraData\u001b[39m: unpack(b) received extra data."
     ]
    }
   ],
   "source": [
    "doc = nlp(content)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When SCONJ advmod\n",
      "Sebastian PROPN compound\n",
      "Thrun PROPN nsubj\n",
      "started VERB advcl\n",
      "working VERB xcomp\n",
      "on ADP prep\n",
      "self NOUN npadvmod\n",
      "- PUNCT punct\n",
      "driving VERB amod\n",
      "cars NOUN pobj\n",
      "at ADP prep\n",
      "Google PROPN pobj\n",
      "in ADP prep\n",
      "2007 NUM pobj\n",
      ", PUNCT punct\n",
      "few ADJ amod\n",
      "people NOUN nsubj\n",
      "outside ADP prep\n",
      "of ADP prep\n",
      "the DET det\n",
      "company NOUN pobj\n",
      "took VERB ROOT\n",
      "him PRON dobj\n",
      "seriously ADV advmod\n",
      ". PUNCT punct\n",
      "“ PUNCT punct\n",
      "I PRON nsubj\n",
      "can AUX aux\n",
      "tell VERB ccomp\n",
      "you PRON dative\n",
      "very ADV advmod\n",
      "senior ADJ amod\n",
      "CEOs NOUN nsubj\n",
      "of ADP prep\n",
      "major ADJ amod\n",
      "American ADJ amod\n",
      "car NOUN compound\n",
      "companies NOUN pobj\n",
      "would AUX aux\n",
      "shake VERB ccomp\n",
      "my PRON poss\n",
      "hand NOUN dobj\n",
      "and CCONJ cc\n",
      "turn VERB conj\n",
      "away ADV advmod\n",
      "because SCONJ mark\n",
      "I PRON nsubj\n",
      "was AUX advcl\n",
      "n’t PART neg\n",
      "worth ADJ acomp\n",
      "talking VERB xcomp\n",
      "to ADP prep\n",
      ", PUNCT punct\n",
      "” PUNCT punct\n",
      "said VERB ROOT\n",
      "Thrun PROPN nsubj\n",
      ", PUNCT punct\n",
      "in ADP prep\n",
      "an DET det\n",
      "interview NOUN pobj\n",
      "with ADP prep\n",
      "Recode PROPN pobj\n",
      "earlier ADV advmod\n",
      "this DET det\n",
      "week NOUN npadvmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokens\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun GPE\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Named Entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n"
     ]
    }
   ],
   "source": [
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
